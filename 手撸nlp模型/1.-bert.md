大家好 欢迎来到这次的自然语言处理教程
在这一门教程里我将会带领大家实现
如何用bert4keras框架来使用bert
预训练模型来进行情感分析的任务
那么这这个教程分三部分
首先第1部分呢就是
如何用一个小的例子
让你们5分钟之内就上手这个框架
看懂这个框架是做什么的
这部分比较简单
因为框架已经实现了大部分的功能
我们只需要调包就可以了
就是俗称的调包侠
然后总的代码大概几行

那么第3部分的话我将会跟大家进入框架内部
看一下如何尽量的不使用框架来做出你的任务
这意味着你要自己写分词
你要自己搭模型
然后 自己搭模型就是意思
你要把每一层的各个参数都给写出来
你要深入Transformer和多头自注意力机制的内部
当然了，我们还是用的预处理模型
而不需要自己去训练
因为如果自己训练的话
可能要用好几张显卡训练一周的时间
而用别人的预处理
我们只需要在自己的特定任务上进行翻译就可以
嗯，那么我们开始吧
首先第1步也是重要一步
就是要下载别人已经训练好的预处理模型
然后这个预处理模型可能在几百兆到几G之间
然后呢 首先导入包
这个导入包之前呢是要把这个包给下载下来
但是它下载我就不明确说了
我默认你已经会怎样下载了
然后下载下之后呢
把文件的路径和各种各样的东西都给附着上
文件夹的路径
这里有那三个文件
首先第1个是config 
第2个是checkpoint
第3个是dictionary
这是你下载下来的文件夹里包含的东西
那么第1步我们先指定一下这个文件夹路径
第2步我们在文件夹路径下
写出这几个东西的地方
下一步呢，我们就是来进行分词
首先我们要根据下载下来的dictionary
来制造制作一个分词器
也就是Tokenizer
那么Toeknizer的对象呢
bert4keras的框架里已经实现了
非常的简单 就是只要tokenizer
= Tokenizer
然后参数传进去
dictionary path
to_lower_case=True就可以了
to_lower_case的意思
就是说把里面的所有词
都从大写换成小写
当然了如果你要深入Tokenizer的内部
你会知道tokenize做的
不仅仅是这些东西
不过这不是现在目的啊
不知道内部原理也可以
然后你就做出了一个model
这个model是什么东西呢
其实它就是一个keras里边的一个类型
嗯，那么我们有了model
我们就可以来进行
把一句话来给变成内部的词语的数量
嗯，那么把一句话变成厄变成里面的隐藏的数量，也就是说把一句话给变成它的特征，这个任务就是叫厄特长提取，呃，那么这个就很简单了，首先呃我我们因为已经有了一个分词器，我们就可以用这个分词器里的音扣的方法，那么简单来说就是硬凑的方法，然后它会返回两个东西，第1个是呃第1个是它的偷看的ID，第2个是它segment的ID，这个东西你们可以先不用去了解，因为这个是呃跟uh but模型里面相关的一些东西，特别是这个c哥们大爱的，他其实用处是如果有两句话的话，那么这个问他你是用来判断这两句话哪个是哪个句话了啊，不过你们都不用管了，然后这输出的是一个什么东西呢，这个偷看你其实是一个list，而这个数字，可能你们现在还不知道是什么东西，都代表了一个呃，偷看你可以简单理解成一个汉字，每个数字对应着一个汉字，然后就比如说中文里面有3000多个汉字，那么就是从一到三千这么呃对对对应了3000个数啊，不过这个是不对的啊，也因为我是为了帮助你们理解，实际上比这个要复杂，然后这个新闻他一定你们现在先不用管，就像我刚才说的，然后我们把这两个呃，符号给传入进这个目的，然后用我们的predict的方法，用predict方法呢，那么实际上就能形成一些向量了，我们现在可以形成呃形成的向量的维度呢，应该就是我们现在是不是有一句话呀，一句话那么第1位就是一，然后我们是不是有4个字呀，那么我们4个字价位就是4，然后我们这个中间每个每个偷看也就是每个词，他会对应着一个很大的一个厄很大的一个向量，那么这个向量呢就是它的特征，厄这个这个维度取决于你的模型，厄比如说波尔贝斯它是768，然后因为我们现在用的是肉包它，所以它是1024，那么所以我们它会输出一个1×4×1024的向量，是不是这样的，其实不是，我们现在是不是有4个字等于是个偷看呀，那么实际上真的前面还会加一个cls，在后面还会加一个sep偷看，那么为什么这个回家偷看呢？这实际上就是标志着一句话的开始和一句话的结尾，啊，当然了，其实还没有还是没有，像像我刚才说的还是没有这么简单，不过你我现在为了你理解你知道知你知道到这里就可以了，那么所以实际上它会输出1×6×1024，那么让我们来一起来运行一下看看吧，我现在电脑的配置是I77700k，加上1050啊，11050它不过我并没有用到显卡，我是用的CPU跑的，所以呃你们的电脑应该也是可以跑的，因为这个任务对计算资源要求并不复杂，那么运行一下，看看一看吧，